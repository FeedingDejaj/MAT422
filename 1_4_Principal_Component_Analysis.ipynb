{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeedingDejaj/MAT422/blob/main/1_4_Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c5ece9",
      "metadata": {
        "id": "11c5ece9"
      },
      "source": [
        "# Principal Component Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Abdula Alkhafaji with assistance from Glendale Community College Tutoring Center"
      ],
      "metadata": {
        "id": "H8S02ReRvLmL"
      },
      "id": "H8S02ReRvLmL"
    },
    {
      "cell_type": "markdown",
      "id": "fe112c27",
      "metadata": {
        "id": "fe112c27"
      },
      "source": [
        "This notebook covers the fundamental concepts of Principal Component Analysis (PCA), Singular Value Decomposition (SVD), lower-rank matrix approximations, and other key concepts related to PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cc0396",
      "metadata": {
        "id": "a6cc0396"
      },
      "source": [
        "## 1.4 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "931666bc",
      "metadata": {
        "id": "931666bc"
      },
      "source": [
        "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while retaining as much variability as possible. It works by transforming the original variables into a new set of variables called principal components, which are uncorrelated and ordered by the amount of variance they capture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449b732",
      "metadata": {
        "id": "c449b732"
      },
      "source": [
        "### 1.4.1 Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b081b7",
      "metadata": {
        "id": "f0b081b7"
      },
      "source": [
        "Singular Value Decomposition (SVD) is a mathematical method used to decompose a matrix into three other matrices: a diagonal matrix of singular values and two orthogonal matrices. It is commonly used in PCA to compute the principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58965d5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58965d5e",
        "outputId": "bbf40d4e-9e5d-4d74-f2aa-06cd878aeff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " [[0.4616197  0.7277056  0.17053715]\n",
            " [0.72645923 0.8955592  0.21996973]\n",
            " [0.33779387 0.60560473 0.05160472]]\n",
            "\n",
            "U matrix:\n",
            " [[-0.54241088  0.22227531 -0.81017537]\n",
            " [-0.724317   -0.61230156  0.31694114]\n",
            " [-0.42562345  0.75873611  0.49311681]]\n",
            "\n",
            "Singular values (S):\n",
            " [1.61708905 0.12654894 0.04919661]\n",
            "\n",
            "Vt matrix:\n",
            " [[-0.5691383  -0.80462097 -0.16931238]\n",
            " [-0.67886025  0.5760054  -0.45537517]\n",
            " [ 0.46392926 -0.14423201 -0.87405193]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a random 3x3 matrix\n",
        "A = np.random.rand(3, 3)\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "print('Matrix A:\\n', A)\n",
        "print('\\nU matrix:\\n', U)\n",
        "print('\\nSingular values (S):\\n', S)\n",
        "print('\\nVt matrix:\\n', Vt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5533d92c",
      "metadata": {
        "id": "5533d92c"
      },
      "source": [
        "### 1.4.2 Lower Rank Matrix Approximations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d242f1f",
      "metadata": {
        "id": "0d242f1f"
      },
      "source": [
        "A lower-rank matrix approximation is an approximation of a matrix using a smaller number of singular values. This technique is used in PCA to reduce the dimensionality of the data by selecting only the most important components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "118b6f21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "118b6f21",
        "outputId": "2ad9fd2c-43d5-452c-bf8e-5b3683f6e50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1 approximation of A:\n",
            " [[0.4992064  0.70575453 0.14850841]\n",
            " [0.6666232  0.94244054 0.19831307]\n",
            " [0.3917214  0.5537973  0.11653281]]\n"
          ]
        }
      ],
      "source": [
        "# We'll keep only the top singular value for approximation\n",
        "rank_1_approx = U[:, 0].reshape(-1, 1) @ np.diag([S[0]]) @ Vt[0, :].reshape(1, -1)\n",
        "print('Rank 1 approximation of A:\\n', rank_1_approx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b333ad0",
      "metadata": {
        "id": "2b333ad0"
      },
      "source": [
        "#### 1.4.2.1 Induced Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f523f8bb",
      "metadata": {
        "id": "f523f8bb"
      },
      "source": [
        "The **induced norm** is the largest singular value of a matrix. It measures the maximum amount by which the matrix can stretch a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75055e05",
      "metadata": {
        "id": "75055e05"
      },
      "source": [
        "### 1.4.3 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281c5828",
      "metadata": {
        "id": "281c5828"
      },
      "source": [
        "PCA transforms data into a new coordinate system such that the greatest variance comes to lie on the first principal component, the second greatest variance on the second component, and so on. This allows for dimensionality reduction while preserving as much variance as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68289b20",
      "metadata": {
        "id": "68289b20"
      },
      "outputs": [],
      "source": [
        "#Example from Internet\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random 2D dataset\n",
        "X = np.random.rand(100, 2)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot the original data and the principal components\n",
        "plt.scatter(X[:, 0], X[:, 1], label='Original Data')\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], label='PCA Transformed Data')\n",
        "plt.legend()\n",
        "plt.title('PCA on 2D Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ef05474",
      "metadata": {
        "id": "1ef05474"
      },
      "source": [
        "#### 1.4.3.1 Covariance Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee3e313",
      "metadata": {
        "id": "4ee3e313"
      },
      "source": [
        "The **covariance matrix** is used in PCA to understand the relationships between variables. The elements of the covariance matrix represent the covariance between pairs of variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5c61d254",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c61d254",
        "outputId": "998cc54c-655d-4116-a0ad-13643402c353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance matrix:\n",
            " [[0.08299502 0.00615148]\n",
            " [0.00615148 0.07306044]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Added import statement\n",
        "\n",
        "# Create a random 2D dataset\n",
        "X = np.random.rand(100, 2) # Defined X\n",
        "\n",
        "cov_matrix = np.cov(X.T)\n",
        "print('Covariance matrix:\\n', cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d893cd",
      "metadata": {
        "id": "e4d893cd"
      },
      "source": [
        "#### 1.4.3.3 Total Variance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49a5607",
      "metadata": {
        "id": "c49a5607"
      },
      "source": [
        "The **total variance** in PCA is the sum of the variances of the principal components. It provides a measure of how much information is retained after dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "842d4084",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "842d4084",
        "outputId": "43e5847e-4756-4432-b6d4-f27a90981542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance matrix:\n",
            " [[0.07952653 0.00754066]\n",
            " [0.00754066 0.09535567]]\n",
            "Explained variance ratio: [0.56250889 0.43749111]\n",
            "Total variance explained: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a random 2D dataset\n",
        "X = np.random.rand(100, 2)\n",
        "\n",
        "cov_matrix = np.cov(X.T)\n",
        "print('Covariance matrix:\\n', cov_matrix)\n",
        "\n",
        "# Create a PCA object and fit the data\n",
        "pca = PCA(n_components=2) # Create PCA object\n",
        "pca.fit(X) # Fit the data to the PCA object\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "total_variance = np.sum(explained_variance)\n",
        "print('Explained variance ratio:', explained_variance)\n",
        "print('Total variance explained:', total_variance)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}